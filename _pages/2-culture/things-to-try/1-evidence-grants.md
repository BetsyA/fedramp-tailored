---
Title: Tiered-Evidence Grant Programs
issue: 9
---

#Things to Try: Tiered-Evidence Grant Programs

Among the most notable recent advances in federal grant making are "tiered-evidence" or “innovation fund” grant designs, which allow programs to allocate resources to practices in accordance with their level of evidence of effectiveness. A key challenge for agencies is how to structure investments to support promising new ideas while also investing in the scale-up of approaches that have credible results and the potential for broader impact. A tiered-evidence approach provides agencies a systematic way of addressing this challenge.  Here, grants are awarded to programs according to their evidence of effectiveness, and  approaches with stronger evidence of success progressively receive more funding for expanded implementation.  Taking a tiered approach   focuses resources into cost-effective, high-impact interventions. This is particularly important in constrained budgetary environments.  

A tiered-evidence framework builds the evidence base by replicating and further testing proven practices, validating promising approaches, adapting and testing interventions for new areas, and piloting innovative models.  To achieve this,   a strong evaluation component is included. This can be done by requiring local program evaluations  as part of the grant award; national evaluations carried out by third-party researchers, or a combination of both. While individual definitions of "replication," “adaptation,” or “strong evidence” vary across each program, generally speaking, greater investments are made for program models that demonstrate a strong evidence of effectiveness based on program goals ([youth.gov](http://youth.gov/evidence-innovation/investing-evidence)). Tiered or staged funding maximizes taxpayer investment by “failing fast” if necessary and scaling only once evidence of impact has been established.

Tiered-evidence grant programs are just one application of a broader effort to infuse evidence and innovation into policy and program decision-making.  It has been a remarkably effective and innovative tool, as evidenced by the experiences at the U.S. Agency for International Development (USAID)’s Development Innovation Ventures (DIV) Program, the Department of Education’s Investing in Innovation Fund (i3), and the Corporation for National and Community Service’s Social Innovation Fund (SIF).  

## Why Take a Tiered Grantmaking Approach

Tiered grantmaking presents a clear message to grantees and prospective applicants about the importance of building an evidence base. It allows for new ideas to percolate up from different ecosystems, be tried out, scaled and tested, while advancing understanding about a particular policy issue. Tiered grantmaking benefits the overall field of funders - whether government or philanthropic - by allowing them to see how and whether different funded strategies create the desired impact.  For agencies, it’s a valuable way of directing investments towards programs and projects that provide greater impact for each dollar. 

## How to Bring Tiered Grantmaking into Government

Agencies can structure grant competitions in three different "tiers" -- 1) promising proof of concept, 2) validation, and 3) scale-up -- with each successive tier incorporating greater funding and higher evidence requirements. The distribution of a three-tier model can be understood as:

* **_Highest tier_****:** For programs where the evidence base is "strong", that is, they have been proven effective through multiple random assignments or strong quasi-experimental studies that can be replicated with fidelity. These *projects are deemed suitable for scaling* are funded at the highest level because they have been shown to work.

* **_Middle tier:_**** **For programs with only a moderate evidence base, that is limited quasi-experimental studies or a single or small random assignment study. Moderate level funding is provided for *replication grants* designed to further test and validate effectiveness.

* **_Lowest tier:_**** **Where there is only preliminary evidence or a strong theory of action, funding is offered for *development *or *proof of concept *projects with an appropriate evaluation design to determine whether the project would merit further development or replication (*[StratInnov 201*5](http:///h)*). *

The goal of the approach is to identify evidence-based models that are replicable and bring them to scale. Grantees receive a base level of funding under the "proof of concept" (lowest tier) and additional funding may be awarded as evidence of effectiveness and impact is gathered.  This framework enables agencies to direct more dollars towards successful, scalable programs. At the same time, it is a useful vehicle to seed and fund potential interventions that require further testing and validation ([RFA](http://results4america.org/policy-hub/invest-works-fact-sheet-federal-evidence-based-innovation-programs/)).  Larger investments in ineffective programs are avoided, while the built-in mechanism for scaling up interventions that work also helps prevent the troubling problem of not investing in programs with proven high returns. ([2014 Budget, Ch. 7](https://www.whitehouse.gov/sites/default/files/docs/erp_2014_chapter_7.pdf))

### Implications

Taking a tiered-evidence approach to grantmaking has implications for leaders, policymakers, and career civil service employees Problems across social, economic, and environmental domains are often at varying levels of scientific understanding. Engaging existing researchers in-house or in the broader scientific community through the tiered model builds a foundation for solving big problems using evidence. Senior leaders and career employees looking to introduce an evidence-driven approach to grantmaking should adequately understand the existing state of knowledge, define and focus their efforts, and ensure that proposed approaches are empirically validated by experienced researchers using quantitative scientific methods.

# Benefits of Tiered-Evidence Grant Programs

With nearly $500 million shifted toward federal evidence-based programs between FY14 and FY15, agencies are moving towards funding "what works."[ [Source]](http://results4america.org/wp-content/uploads/2015/04/RFA_Local-Moneyball_j.pdf) The FY17 federal budget invests nearly $700 million in discretionary tiered-evidence programs, nearly double the funding from related programs in 2016.[ [Source](https://www.whitehouse.gov/sites/default/files/omb/budget/fy2017/assets/fact_sheets/Building%2520and%2520Using%2520Evidence.pdf)] 

There are multiple benefits to deploying a tiered-evidence approach for federal grant programs. Compared to traditional approaches in federal grantmaking, the tiered method enables policymakers to focus funding on what works. Administering grants in this fashion is rooted in the idea that success is what matters most in funding interventions. The goal of tiered grants is to help programs move up from pilot to scaled interventions or policies based on greater evidence, thus enabling more funding to be steered toward successful programs. The chart below compares how the benefits of tiered grantmaking compare to traditional approaches ([RFA](http:///h)).

<table>
  <tr>
    <td>Comparison of Traditional Federal Grant Programs and Evidence-Based Innovation Programs</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td> </td>
    <td>Traditional Grants</td>
    <td>Evidence-Based Innovation Programs</td>
  </tr>
  <tr>
    <td>Incentives for grantees to do what works</td>
    <td>Grantees rarely have incentives to develop evidence-based approaches.</td>
    <td>Grantees are funded according to the success of their model.</td>
  </tr>
  <tr>
    <td>Learning and feedback loops</td>
    <td>Grant processes are rarely designed to produce evidence that can be used to inform future investments or program models.</td>
    <td>The rigorous evaluations required by evidence-based innovation funds generate shared data that can be used to scale effective approaches and inform future investments.</td>
  </tr>
  <tr>
    <td>Innovation</td>
    <td>Traditional grants rarely award untested approaches regardless of potential.</td>
    <td>By design, promising innovative ideas are funded, and once validated, are expanded for wider application.</td>
  </tr>
  <tr>
    <td>Incentives for producing evidence </td>
    <td>By not awarding larger grants to evidence-supported programs, investors and providers are less incentivized to further develop evidence-supported programs.</td>
    <td>By using the tiered-evidence model, grantees are incentivized to produce evidence to advance program development, leading to more reliable investments.</td>
  </tr>
  <tr>
    <td>Resources</td>
    <td>Funds awarded by traditional grants rarely cover the cost of rigorous evaluation.</td>
    <td>The coverage of cost for program funding, staff, time, and rigorous evaluation is included within evidence-based innovation funds.</td>
  </tr>
  <tr>
    <td>Leveraging "the crowd"</td>
    <td>Traditional grant programs are often less able to capitalize on innovative ideas from outside actors.</td>
    <td>Tier-based grant programs naturally harness innovative ideas by funding promising programs at various stages of development.</td>
  </tr>
  <tr>
    <td>Mobilizing Capital</td>
    <td>Limited public funds are tapped to finance traditional grant programs.</td>
    <td>Through fundraising strategies such as match-requirements, public and private dollars can be combined to expand budgets, including through philanthropic sources.</td>
  </tr>
  <tr>
    <td>Enabling Intermediaries and Grantmaking  Experts</td>
    <td>Traditional grants are awarded in a top-down fashion through a two-way model.</td>
    <td>By working through intermediaries, evidence-based innovation funds capitalize on the expertise of well-positioned grantmaking intermediaries to more effectively disperse funding.</td>
  </tr>
</table>


**_Note_***: This chart is derived from Results for America.*

## Measuring Impact

What does it mean to say that a program worked? What counts as evidence, and how is scientific "rigor" understood? Tiered grantmaking is accompanied by sound evaluation practices that assess the success of a program or intervention.

A commonly accepted method for testing interventions are Randomized Control Trials (RCTs).  Participants from a given population are selected at random to receive an intervention (i.e. the A group). The effects of the intervention are compared to a control group from the same population (the B group) to determine its effectiveness; the comparison of the outcomes gauges the impact of the intervention against what would have happened if nothing had changed. Success is achieved when the outcomes from the A group are superior to those in the comparison B group in one or more important ways. For example, poor and low-income mothers participating in a home visiting intervention were found to have better outcomes in several areas than similar mothers who were not participating in the home visit program. Improvements included less smoking and drinking, reduced rates of child abuse, and healthier children (Haskins and Margolis, 2015). 

![image alt text](image_0.png) 

*Source: June 2014 Nesta report: "i-teams: The teams and funds making innovation happen around the world**[.*"](http://theiteams.org/system/files_force/i-teams_June%25202014.pdf)

The RCT is often considered the gold standard for testing interventions. Within the tiered grantmaking approach, it is often used to determine whether interventions should be brought to scale.  

As programs move across the three tiers of grantmaking, different forms of evaluation are appropriate to help determine what is/is not working. The RCT is ideal for moving interventions from tier 2 to 3, but earlier in the process other forms of evaluation can be utilized to produce evidence on whether a proof of concept may be successful. Indeed, RCTs are not always feasible or appropriate for a given intervention. The key to building scientific evidence of effectiveness is to use the most rigorous methodology feasible for answering a given question about the program in question. "Rigor" in this context can be defined as “credible, useful, and unbiased” ([Zandniapour and Brennan, 2010](http://www.evaluationinnovation.org/publications/newsletter/issue-archive/2010/sep/advocacy-evaluation-and-rigor)). Quasi-experimental designs or other defined comparative methods can also provide evidence of effectiveness. 

The main consideration for grantors and policymakers is creating evaluations with research designs that instill confidence in the results. Jim Manzi, an expert on running RCTs in the private sector, has observed, "The percentage of government policy programs that can be replicated with well-structured randomized trials to create statistically significant improvements** **is under 10% of those attempted. Rather than thinking, ‘I've got to get a RCT for my program to prove it works,’ policymakers and grantors should focus on radically ramping up the number of trials and rigorous testing – because most of the things we are trying are not going to work."  (GovInnovator podcast, interview with Jim Manzi, 9:00 min). 

# Context for Tiered-Evidence Grant Programs

The context for using tiered-evidence grantmaking varies significantly by agency and domain of work, but several common core attributes demonstrate important conditions for deployment. One common fixture across existing users is that they use the tiered grantmaking framework to continue important mission work while simultaneously building an evidence base to support future funding and program decisions.  

A three-tiered model is suitable to a wide range of programs and missions, with the caveat that they have regulatory flexibility, sufficient funds, and evaluation capacity to implement successfully.  However, many existing programs can still adopt aspects of this grantmaking approach if they wish to increase their investment in evidence-backed initiatives. For example, it’s appropriate to consider for any agency that could benefit from a systematic investment approach that encourages innovation while creating a pathway for further investment in projects found effective by strong evaluations. Even when there are few or no interventions that meet the evidence standards for validation or scale-up grants, a program can adopt the three-tiered framework to signal the expectation that some interventions testing a proof of concept will, over time, become eligible for validation and scale-up grants.

Several agencies and departments - ranging from the Department of Education to the U.S. Agency for International Development - have implemented evidence-based grant programs that apply a tiered framework to assess the evidence supporting a proposed project and to determine appropriate funding levels. Again, programs backed by stronger evidence, as established in a rigorous agency review process, are eligible for more funding. All programs are expected to evaluate their results. Examples of tiered-evidence programs include the Department of Education's[ Investing in Innovation](http://www2.ed.gov/programs/innovation/index.html) program and the Department of Health and Human Services'[ Teen Pregnancy Prevention and Home Visiting](http://mchb.hrsa.gov/programs/homevisiting/) programs.

## History of emergence of tiered-evidence approaches within government [Sidebar] 

Programs in government have historically lacked an evidence-based approach in determining what to fund. The introduction of tiered grantmaking was sparked by this noted lack of evidence to support informed decision-making about social policy. A core group of innovators  within the Office of Management and Budget (OMB) observed the effects of big data and strong evaluations outside the public sector. They began to wonder, "What can we do to help our programs improve? We started thinking about how to create program designs and structure budgets that guaranteed people would test things," shared Kathy Stack, former OMB deputy associate director.

In 2008, OMB reviewed 115 different program evaluations from across government. While the majority of these agency-led evaluations had concluded that a program was working well, the standards for effectiveness were not clearly articulated or data-driven. Out of the 115 program evaluations that were reviewed, only ten used rigorous designs, and of those ten, only three demonstrated a positive impact. OMB concluded programs were either not using evidence to make decisions on program effectiveness, or agencies were using very different definitions of success. 

Seeing that agencies lacked a consistent and informed approach to evaluations, OMB saw an opportunity to leverage its guidance to systematize and encourage evaluation across government agencies.  For OMB officials, the thinking was clear: Standards were needed to define what strong evidence looked like, and programs needed to develop capacity for building evidence-based programming. The first priority was establishing an understanding of evidence as a continuum. By focusing first on improving agency staff’s understanding of evaluation methods and how to interpret results, programs would be better positioned to assess their existing programs and formulate a means for  validating current and future interventions. 

Within OMB, a "coalition of the willing" began to strategize around how things could be done differently. Out of these conversations, an “evidence agenda” emerged that focused on how to build incentives into grantmaking to using existing proof of effectiveness. Conversations with the research community, leaders within agencies, and in-house evaluation teams, began laying a blueprint for taking a tiered approach.  A deeper process of cultural change developed by working with agencies and departments to embed evaluation directly into their core strategies. The Department of Education was one of the first to begin strategizing about how to deploy the method across its portfolio. The “evidence agenda” advanced to a “learning agenda”, and as more programs adopted a tiered approach, core guidelines for evaluating programs became standardized. The way agencies across government described strong evidence improved, and more data-driven approaches were adopted. 

## The "Learning Agenda" Approach

The Administration encourages agencies to adopt "learning agenda" approaches in which agencies collaboratively identify the critical questions that, when answered, will help their programs work more effectively and develop a plan to answer those questions using the most appropriate tools. 

The key components of this learning agenda approach are that agencies: 

* Identify the most important questions that need to be answered in order to improve program implementation and performance. These questions should reflect the interests and needs of a large group of stakeholders, including program office staff and leadership, agency and Administrative leadership, program partners at state and local levels, and researchers, as well as legislative requirements and Congressional interests. 

* Strategically prioritize which of those questions to answer within available resources, including which studies or analyses will help the agency make the most informed decisions. 

* Identify the most appropriate tools and methods (e.g. evaluations, research, analytics, and/or performance measures) to answer each question.

* Implement studies, evaluations, and analysis using the most rigorous methods appropriate to the context. 

* Develop plans to disseminate findings in ways that are accessible and useful to program officials, policy-makers, practitioners, and other key stakeholders—including integrating results into performance 

Source: OMB, "[Building the Capacity to Produce and Use Evidence](https://www.whitehouse.gov/sites/default/files/omb/budget/fy2017/assets/ap_7_evidence.pdf)"

# Success Stories

**Case Studies: **

1. [USAID: Development Innovation Fund (DIV)](#heading=h.ljrbqcwbcbwy)

2. [ED Investing in Innovation Fund (i3)](#heading=h.p5nzuzdxmkrz)

3. [CNCS: Social Innovation Fund (SIF)](#heading=h.z5tcbw5twny6)

**Abbreviated Case Focuses:**

1. HHS:  Teen Pregnancy Prevention Program

2. HHS: Maternal, Infant and Early Childhood Home Visiting Program

3. DOL: Workforce Innovation Fund (WIF)

4. DOL:  TAACCCT (comm college + training) 

5. ED: First in the World (FITW)

## [The U.S. Agency for International Development (USAID) Development Innovation Ventures (DIV) Program](http://www.usaid.gov/div/) 

### Summary 

Begun in 2010, t Development Innovation Ventures (DIV) aims to identify, support, and scale solutions to the world’s most important development challenges, including economic growth, agriculture, trade, global health, democracy, conflict prevention, and humanitarian assistance. 

The DIV model is designed to find breakthrough solutions, minimize risk and maximize impact through stage financing, rigorously testing impact and cost effectiveness, and focus on scaling proven solutions through the public or private sectors. Through this model, DIV seeks to advance innovations that work while avoiding long term investments in those that don’t.

### Key Accomplishments and Impact 

In 2015, DIV funded more than 150+ grants  in 24 countries, generating positive impact in eight issue areas around the globe: (1) Saving Lives, (2) Lighting the World, (3) Bringing Food to the Table, (4) Lifting People out of Poverty, (5) Helping Youth Thrive, (6) Improving Government Accountability, (7) Promoting Healthy Habits, and (8) Ensuring Access to Safe Drinking Water.  [[Source]](http://results4america.org/policy-hub/invest-works-fact-sheet-federal-evidence-based-innovation-programs/)

**[Graph depicting level of DIV investment across three tiers]**

![image alt text](image_1.png)

**[[Image source**]](https://www.usaid.gov/div/about)

### Notable DIV Accomplishments 

* Over their first three years, 3,167 proposals were submitted to DIV for "Proof of Concept" funding aimed at tackling DIV's priority areas. Seventy percent of the proposals were derived from organizations that had not previously requested funding from USAID, demonstrating how the tiered approach functions as a catalyst to source and test new ideas for complex social challenges.

* The average DIV grantee brought $0.65 in cost-share for every $1 of funding from USAID. The model helped bring a diverse portfolio of actors into the development tent.  One third of partners come  from the private sector, 52% from international NGOs, and 13% from academic institutions. According to USAID, the inclusion of academia supports an emphasis on scientific rigor in evaluations. 

* 58% of all DIV partners during the first three years of the work conducted randomized control trials to improve the evidence base and understanding of the particular development issue. 

(See[ https://www.usaid.gov/sites/default/files/div_yearbook.pdf](https://www.usaid.gov/sites/default/files/div_yearbook.pdf))

### How They Did It  

DIV selects, tests, and scales projects based on three main criteria: 

1. Cost-effectiveness: Seek ideas that can deliver greater development impacts per dollar than standard practice.

2. Rigorous testing: Use rigorous evaluation tools to identify what works and what does not, and scale only those solutions that are proven to produce development outcomes. 

3. Pathways to scale: Innovations are expected to eventually scale up through private sector, public sector, or, in some cases, a combination of the two in order to reach financial sustainability without long-term DIV support. An exit strategy with multiple institutional partners is key to the overall approach. 

Based on these criteria, DIV applies the three-tiered approach to grantmaking:

* **Stage 1 grants** support the initial testing of a development idea in order to prove the viability and impact of the concept. Stage 1 grants provide $25,000 to $150,000 in project funding that may be used for up to two years.

* **Stage 2 grants** support ideas that are ready to be measured for overall impact, sustainability and possible scale. Stage 2 grants range from $150,000 to $1,500,000 and may be used over the course of three years.

* **Stage 3 grants** support proven ideas that are ready to be scaled, potentially across multiple countries. Stage 3 grants range from $1,500,000 to $15,000,000 in funding for up to five years.  ([https://www.usaid.gov/div/model](https://www.usaid.gov/div/model))

To operationalize the criteria across these three stages, DIV:

1. Runs a year round competition to source breakthrough solutions, using the above criteria for funded grants, emphasizing cost effectiveness, rigorous testing, and pathways to scale

2. Applies a three-tiered staged financing model to invest in and test ideas in various stages of their growth. Applicants can apply to any stage, and must re-compete to advance to the next stage. Further funding is offered for promising interventions driven by successful, rigorous evidence. 

3. Rigorously evaluates impact and cost-effectiveness. As interventions advance through the different tiers of funding, more evidence is needed. Using larger populations and impact evaluations to test the interventions, DIV looks for evidence at scale, while also maintaining a focus on cost and long-term financial sustainability of the intervention beyond the DIV portfolio. 

### Key Insights

* **Change the donor relationship**: "In some respects, DIV is more like a venture capital approach," notes Andy Feldman, previously at OMB. The DIV model is indeed inspired by how venture capitalists invest and pool resources across various startups and entrepreneurial ventures except their goal is strictly social, economic, and environmental impact. By focusing on cost-efficiency and multiple partnerships, the approach has enabled USAID to develop a model that is viable for long-term growth while avoiding the traditional donor fatigue that occurs in traditional global development projects. 

* **Good ideas can come from anywhere: **DIV is the quintessential example of how to harness the power of ideas by embracing the power of the crowd for sourcing creativity. Accepting that there is a role within the portfolio of government tools for bottom up solutions is a key takeaway from USAID’s approach. 

* **Engage the entire ecosystem:** It’s not sufficient to crowdsource proposals and solutions from the ecosystem; follow-on engagement is critical. DIV has a roster of over 300 people inside the agency plus hundreds of external experts outside it in various sectors and countries around the world. Engaging internal staff and human resources is key because they have relationships with people in these external sectors. This broader ecosystem, as well as internal support, is key for due diligence of proposals and finding the very best of the ideas.

* **Infuse competition**: Former Managing Director of DIV, Jeffrey Brown explained that one of DIV’s secrets is not picking winners while working to figure out how to measure their impact. Their approach instead focuses on embracing all "three legs of the stool": 

![image alt text](image_2.png)

As Brown explains, "Before you even pitch an idea  […] go out and find somebody who can help you measure it." Investment is made when there is promise that scale, cost-effectiveness, and testing can promote the initiative; there is explicitly no ”picking winners.” Brown noted that while DIV did not mandate evidence standards,  “The level of the competition has really driven people to find this is a convenient way to focus on using evidence to demonstrate what works.” 

### Additional Resources

* USAID DIV[ Yearbook](https://www.usaid.gov/sites/default/files/div_yearbook.pdf)

* USAID DIV[ Fact Sheet](https://www.usaid.gov/news-information/fact-sheets/development-innovation-ventures-june-2016)

* USAID DIV[ Application Materials](https://www.usaid.gov/div/apply)

## [The Department of Education’s Investing in Innovation (I3) Program](http://www2.ed.gov/programs/innovation/resources.html#tools)

### Summary

The Investing in Innovation Fund was established under section 14007 of the American Recovery and Reinvestment Act of 2009 (ARRA). The[ purpose](http://www2.ed.gov/programs/innovation/index.html) of this program is to provide competitive grants to applicants with a record of improving student achievement and attainment in order to expand the implementation of, and investment in, innovative practices that are demonstrated to have an impact on improving student achievement or student growth, closing achievement gaps, decreasing dropout rates, increasing high school graduation rates, or increasing college enrollment and completion rates. 

The Department of Education's (ED) Investing in Innovation Fund (i3) invests in high-impact, potentially transformative K-12 education interventions, ranging from new ideas with significant potential to those with strong evidence of effectiveness that are ready to be scaled up. The goal of i3 is to accelerate the development of innovative practices and to expand the implementation of practices that have a demonstrated impact on improving student outcomes. In FY15, i3 was funded at $120 million ([RFA](http://results4america.org/wp-content/uploads/2015/10/Innovation-fact-sheet.pdf)). 

### Key Accomplishments and Impact 

Through i3, the ED has funded more than $1 billion in tiered-evidence grants to improve educational achievement, attainment, growth, or to close achievement gaps.[ [Source]](http://www2.ed.gov/programs/innovation/index.html) The program has received positive reviews by applicant organizations. Foundations have indicated their support for i3 by creating an easy-to-use online platform for philanthropic organizations ([Foundation Registry i3](https://www.foundationregistryi3.org/)) to identify i3 applicants and seek support from foundations willing to match grants. It also informs foundations about active initiatives they may want to invest in or collaborate with. Through the platform, grantees have raised over $84 million in private funds through the Registry {[Source](http://www2.ed.gov/programs/innovation/i3securing.pdf)].

In November 2015, Results for America reported on nine[ impact evaluations](http://www2.ed.gov/programs/innovation/awards.html) of i3 projects. Examples indicate success in a variety of settings and educational focus areas:

* A 2015 [evaluation](http://www.mathematica-mpr.com/~/media/publications/pdfs/education/kipp_scale-up_vol1.pdf) of KIPP charter schools found positive, statistically significant, and educationally meaningful impacts on 1) reading and math achievement in elementary grades, 2) math, reading, science, and social studies achievement in middle grades, and 3) student achievement for students new to KIPP high schools. 

* A 2015 [report](http://www.mdrc.org/publication/scaling-success-all-model-school-reform) on Success for All (SFA) found that SFA is an effective vehicle for teaching phonics at the second grade level and that students entering kindergarten with low pre literacy skills registered statistically significantly higher scores on measures of phonics in grades K-2, word recognition, and reading fluency than similar students in control groups in grades K-2.

* A 2015 [evaluation](https://www.mathematica-mpr.com/~/media/publications/pdfs/education/tfa_investing_innovation.pdf) of Teach For America (TFA) found that first- and second-year corps members in elementary grades were as effective as other teachers (with an average of 14 years experience) in the same high-poverty schools in reading and math, and a sub-analysis focused on grades pre-K-2 showed students of TFA teachers gained an additional 1.3 months on measures of reading skill relative to other students in the same schools. 

* Findings from a final [report](http://www.cli.org/wp-content/uploads/2015/09/CLI-i3-Impact-Report-July-2015.pdf) on the Children’s Literacy Initiative (CLI) demonstrate that kindergartners and second graders scored significantly higher on early reading tests than other students, and that CLI had a statistically significant positive impact on the quality of teachers’ literacy instruction in grades K-1.

According to the Education Department, in grants made between 2010 and 2013, all 35 Validation grantees were on track to have evaluations that meet their required standards from[ What Works Clearinghouse](http://ies.ed.gov/ncee/wwc/) (WWC) and 76 of 77 i3 Development grants will produce emerging evidence for improving student outcomes, with a majority meeting WWC standards. [[RFA](http://results4america.org/wp-content/uploads/2015/10/Innovation-fact-sheet.pdf)]

### How They Did It

All i3 grantees are required to embed rigorous third-party evaluations within their design to determine their impact, relevant lessons about program design and implementation, and ultimately to identify practices that should be scaled (RFA). The i3 program awards grants to school districts and nonprofit organizations in partnership with school districts/schools, and all grantees must obtain matching funds from the private sector.

The i3 program uses a useful and replicable three-tier evidence framework to direct larger awards to projects with the strongest evidence base and to support promising projects that undertake a rigorous evaluation, as outlined on the ED’s i3[ website](http://www2.ed.gov/programs/innovation/awards.html):

* **Development grants** fund the development and testing of evidence-based practices that merit systematic study. These grants support new or proven practices for addressing widely shared challenges in education. Since 2010, 98 development grants have been awarded, ranging from up to $3 million to $5 million each.

* **Validation grants** fund the expansion of projects that are backed by moderate evidence, to either the regional level or national level. Since validation grants were initiated in 2013, 39 grants have been awarded, up to $12 million each.

* **Scale-up grants** fund the expansion of programs with strong evidence of effectiveness. Since 2010, six Scale-up grants have been awarded, ranging from up to $20 to $50 million each

#### Grant Types Available Within the i3 Program

![image alt text](image_3.png)

[[Image Source]](http://www.ewa.org/sites/main/files/file-attachments/supporting-scaling-change-i3.pdf)

### Key Insights

* **For senior leadership: Start somewhere and iterate as you learn:** "Don’t let the perfect be the enemy of the good. You always have to start somewhere – the expertise and the capacity of your team to take on this work needs to be developed overtime. The availability of solutions that meet the highest evidence bars would probably be limited early on. Be willing to think about this in terms of stages, both in terms of time and in terms of the levels of evidence that you want to use in making a resource allocation and selection decisions." – Jim Shelton, former ED Deputy Secretary and COO.

* **Leverage your resources to begin instituting a culture of evidence: **Proportionally, i3 is a small amount of funding relative to the overall size of ED’s budget. For ED, Shelton explained, the early emphasis was, "How can we actually drive more of our resources through the lens of things that have a high probability of working? And how can we get the feel to start looking for those things and people who produce products or services that actually feel they have evidence that it can work"? This was central to the approach with I3 as they worked to infuse more data and evidence driven decision-making across the department. The work continues, but it set the foundation for influencing a cultural shift within the department. 

* **View it as a pipeline: **ED focused on building a sustainable pipeline to test what works. By focusing on the programs that are competitive in the beginning, the approach enabled successful interventions to bubble up to the top and push through the tiered-evidence continuum. I3 followed a simple set of systems: programs received with a "little bit" of evidence received proportional funding, and received more money as evidence accrued.  This helped create a system to prioritize large formula dollars. 

### Additional Resources

* Office of Innovation and Improvement –[ Resources on I3 Grants](http://innovation.ed.gov/what-we-do/innovation/investing-in-innovation-i3/fy-2016-competition/)

* [Conversation](http://govinnovator.com/jim_shelton/) with former Deputy Secretary and COO Jim Shelton about i3 

## [The Corporation for National and Community Service’s Social Innovation Fund (SIF)](http://www.nationalservice.gov/about/programs/innovation.asp)  

### Summary 

Authorized by the Edward M. Kennedy Serve America Act in April of 2009, the Social Innovation Fund (SIF) is a program of the Corporation for National and Community Service (CNCS). The SIF’s goal is to: "find what works and make it work for more people." It does so through a network of grantees and subgrantees working to implement innovative and effective evidence-based solutions to local and national challenges in three priority areas: economic opportunity, healthy futures, and youth development. The SIF runs two competitive grantmaking programs:[ SIF Classic](http://www.nationalservice.gov/programs/social-innovation-fund/our-programs/classic), a tiered-evidence investment strategy that leverages public-private partnerships, and[ Pay for Success](http://www.nationalservice.gov/programs/social-innovation-fund/our-programs/pay-success), a program that builds capacity for, pilots, and tests Social Impact Bonds. 

### Key Accomplishments and Impact 

As of March 2016, the SIF and its partners have invested more than $800 million in community solutions. The SIF's private-sector partners have leveraged match fund commitments valued at $627.5 million--more than double the original federal investment of $295 million. The SIF has made a total of 43 awards to intermediary grantees located in 17 states and the District of Columbia. These in turn have funded more than 450 nonprofit organizations. 

* **SIF has increased grantee capacity for evaluation and assessment.** Compared to a National Sample of Grantmaking Nonprofits, SIF 2010-2012 grantees experienced significantly more growth in three areas, including: 1) conducting rigorous evaluations of the programs; 2) using evaluation findings to improve programs; and 3) using evaluation findings to demonstrate and communicate effectiveness of programs funded by the organization. 

* **SIF investment is increasing participating programs’ level of evidence.** According to SIF reports, SIF investment has also significantly increased the level of evidence for participating programs. While only a combined total of 29% of programs had strong or moderate levels of evidence upon starting the SIF, 73% of programs were conducting evaluations that, if successful, will achieve moderate or strong levels of evidence. 

* **SIF is supporting the evidence-based learning agenda through the CNCS Evidence Exchange. **This[ searchable database](http://www.nationalservice.gov/impact-our-nation/evidence-exchange/basic-search) allows practitioner and government or private funders access to impact and implementation evaluation reports for the purpose of strengthening programming across the country. All evaluation reports posted to this clearinghouse have been vetted by CNCS staff and were carried out by independent, third-party evaluators. 

![image alt text](image_4.png)

*[Sourc*e](http://www.nationalservice.gov/sites/default/files/documents/SIF_Report_FINAL_508_2015_REVISED_11-17-15_0.pdf)

### How They Did It

The SIF's Classic program combines public and private resources to grow the impact of innovative, community-based solutions that have compelling evidence of improving the lives of people in low-income communities throughout the United States. Through this program, the SIF makes grants to experienced grantmaking institutions or "intermediaries" that are well-positioned within communities to identify the most promising programs and guide them towards greater impact and stronger evidence of success.  These grants range from $1-$10 million annually for up to five years. 

The intermediaries then match the federal funds dollar-for-dollar and hold open competitions to identify high-performing community-based organizations working in low-income communities that have innovative solutions with evidence of compelling results. At least 80 percent of awarded federal funds must be invested in subgrantee programs.  Once selected, these organizations must also match the funds they receive, and participate in rigorous evaluations of the impact of their programs. 

Using similar tiered-evidence definitions as the Department of Education’s i3, the SIF requires that all interventions have at least preliminary evidence as the threshold for entry. Once funded, programs must build on their level of evidence. A program must conduct a rigorous evaluation by partnering with an independent evaluation team that will help build the evidence supporting its effectiveness and potentially move it to a higher tier of evidence.

The SIF’s funding guidelines make clear that programs with higher levels of evidence should be prioritized for greater expansion, receiving more financial support (i.e. larger grants) so that they can scale up their programs. Scale for organizations with preliminary levels of evidence allows for limited expansion to support evaluation efforts by testing interventions with new populations or locations, whereas scale for organizations with moderate and strong levels of evidence allows for more substantial growth to provide services to larger numbers of people in the current or new geographic area(s).

### Key Insights 

Results from both the[ SIF National Assessment](http://www.nationalservice.gov/programs/social-innovation-fund/knowledge-initiative/sif-classic-national-assessment) and an[ independent review](http://www.socialinnovationcenter.org/wp-content/uploads/2015/07/Social_Innovation_Fund-2015-06-30.pdf) of the SIF Classic program identified several overlapping strengths and recommendations for change. These recommendations can be useful considerations for other programs considering implementing an evidence-based granting system. 

* **Consider modifying the SIF’s matching requirement.** Though public-private partnership and 2-level private match required by SIF’s statute is a significant part of it’s value proposition, grantees and subgrantees reported it to be an onerous burden, especially for smaller or rural-based grantees and subgrantees. Both reports suggested looking for ways to reform or amend these requirements. 

* **Continue to support the SIF evaluation program including providing technical assistance.** Both reports confirmed that grantees found the SIF evaluation program increased their capacity for and use of evaluation. This aspect of the program was lifted up as a strength and should be continued, if not expanded, perhaps with the inclusion of an evaluation planning year. 

* **Provide more support for or reform regulatory requirements. **Both reports acknowledged that SIF provides support for grantees to achieve compliance with program regulations but that the number and complexity of these requirements was onerous and any opportunity to lessen the burden posed by these requirements should be pursued. 

* **Increase collaboration and knowledge sharing, particularly with private foundations and philanthropy. **Grantees and subgrantees recommended that the learning network expand beyond SIF participants to achieve field-wide impact, capacity building, and adoption of evidence-based practices. 

### Additional Resources 

* [Social Innovation Fund Programs](http://www.nationalservice.gov/programs/social-innovation-fund)

* [Reports from SIF funded projects and project evaluations](http://www.nationalservice.gov/impact-our-nation/research-and-reports/evidence-exchange)	

## Other Tiered-Evidence Grant Programs  

### Health and Human Services, Teen Pregnancy Prevention Program 

The [Teen Pregnancy Prevention Program](http://www.hhs.gov/ash/oah/oah-initiatives/tpp/) (TPPP), administered by the U.S. Department of Health and Human Services’ Office of Adolescent Health (OAH), provides federal grants on a competitive basis to support innovative and evidence-based programs that reduce teen pregnancy rates particularly in high-risk communities. Up to 10 percent of funds can be used for training, technical assistance, and evaluation. TPPP was funded at $101 million in FY15.

### Health and Human Services: Maternal, Infant, and Early Childhood Home Visiting Program

The [Maternal, Infant and Early Childhood Home Visiting ](http://mchb.hrsa.gov/programs/homevisiting/)Program (MIECHV), administered by the Health Resources and Services Administration at the U.S. Department of Health and Human Services, supports the development and expansion of evidence-based home visiting service delivery models. MIECHV programs provide a range of health and child development services in-home for families for up to five years. As a provision within the Patient Protection and Affordable Care Act, $1.5 billion was available for from FY10 to FY14. The program was extended for one year in FY15, and extended again in H.R. 2, the "Medicare Access and CHIP Reauthorization Act of 2015," which includes a two-year extension of MIECHV at $400 million annually through FY17.

### The Department of Labor’s Workforce Innovation Fund (WIF) 

[The Department of Labor’s Workforce Innovation Fund (WIF)](http://www.doleta.gov/workforce_innovation/) provides more than $140 million to fund the design and delivery of employment and training services that generate long-term improvements in the performance of public workforce systems. Performance is measured in terms of outcomes for job-seekers and employers, and in terms of cost-effectiveness.

### The Department of Labor’s Trade Adjustment Assistance Community College and Career Training (TAACCCT) Program

[The Department of Labor’s Trade Adjustment Assistance Community College and Career Training (TAACCCT) Program](http://www.doleta.gov/taaccct/) provides community colleges and other eligible institutions of higher education with funds to expand and improve their ability to deliver education and career training programs that can be completed in two years or less, are suited for eligible workers, and prepare participants for employment in high-wage, high-skill occupations. TAACCCT has awarded nearly $2B in past four years. 

### The Department of Education: First in the World

[First in the World](http://www2.ed.gov/programs/fitw/index.html) (FITW) supports the development and evaluation of innovative strategies designed to improve college completion, particularly for high need students. FITW expands the database of evidence-based strategies for postsecondary education and seeks to foster new ideas for making higher education more affordable. FITW was funded at $60 million for FY15. In FY14, ED awarded $75 million through four-year Development grants, $20 million of which was set aside for minority-serving institutions. In FY15, FITW awarded two Validation grants and 16 Development grants.

# Challenges and Lessons Learned 

As with any innovation, context is important to consider in its deployment. Not every agency is ready for evidence-based, tiered grantmaking. It, like any tool, has limitations in its application. Learning early from the experiences of other agencies and departments helps to inform our thinking, and offers further ideas on what to consider before launching a tiered grantmaking strategy.

Key ingredients for successful tiered grantmaking include:

* Buy-in from high-level leadership

* Agency culture 

* Agreeing on standards of evidence across the agency,

* Difficulty and cost of measurements, 

* Overcoming structural barriers / adapting to agency operating contexts.  

## High-level authority as a necessary prerequisite

As with the introduction of any innovation, successfully adopting an evidence-based approach to grantmaking requires both top-down and bottom-up approaches. But the commitment of high-level authority is critical to bringing evidence into grantmaking. Leaders can facilitate the building of a coalition of internal and external stakeholders, acquiring resources to implement a culture of evidence, and finding the right people and personnel to encourage the adoption of evidence-based practices. 

## Shifting agency culture

Introducing an innovation into an agency or department requires some "inherent openness to failing and to managing failure," notes Aneesh Chopra, former U.S. CTO. Across the agency or department, instilling an evidence-based mindset is important to ensure successful integration of tiered grantmaking. This begins with leadership but extends out across the agency. Successful strategies of deploying tiered grantmaking within other agencies and departments include 1) building and engaging communities of practice from across other agencies and departments to support staff and civil service in adapting to change, and 2) creating working groups and cohorts of vital employees to work through the challenges of deploying tiered grants together as a group. Many agencies and departments are open to sharing lessons learned and a vibrant ecosystem of innovators exist across different[ communities of practice](https://www.digitalgov.gov/communities/). 

## Agreeing on standards of evidence

According to Sonal Shah, former director of the Office of Social Innovation and Civic Participation, introducing evidence-based grantmaking and policy into government is all about getting everyone on the same page about evidence. Within a given agency or department, people often bring their own methodological and disciplinary understandings of what it means to measure something, and what it means for an intervention to be successful. Across different institutional settings, there are often multiple position titles (Chief Data Officer, Chief Information Officer, Chief Evaluation Officer) that are designated to deal with evidence and data. This can create confusion. Ensuring that everyone involved is using the same terminology, definitions, and standards of rigor when referring to evaluation is important for establishing a culture of evidence driven policy and open communication across the organization.[ Standards of evidence](https://www.whitehouse.gov/sites/default/files/omb/budget/fy2017/assets/ap_7_evidence.pdf) and terminology are a simple way of creating clarity.  

## Difficulty and cost of measurements

One common critique of rigorous evaluations is the cost. Randomized trials are particularly expensive, as significant human resources are required to implement the intervention and collect data. To overcome this challenge, the low-cost RCT has been introduced. This approach encourages the use of administrative data that has already been collected. In this case, administrative data can function as the comparison group, enabling the costs of data collection to be cut in half. Consider accessing administrative data when seeking ways of keeping costs lower. 

## Overcoming structural barriers and adapting to agency operating contexts

Because running evidence-based programs requires more resources, agencies may wish to combine multiple smaller programs into larger, evidence-based efforts. Moreover, where existing grant programs are unable to be restructured into tiers, there are alternate methods for integrating evidence into discretionary grant programs. For instance, competitions can award *competitive preference points* for those applicants who demonstrate higher levels of existing evidence or propose using greater rigor and evaluation in their project plans. These types of designs are useful when there are many applications of varying quality, and where a streamlined, pre-application process can identify leading proposals. 

# How to Implement Tiered-Evidence Grant Programs

In this section you’ll find three tools helpful for you to implement these kinds of programs:

* Checklist for introducing a tiered approach

* Structuring tiered grants

* Introducing a Chief Evaluation Officer

## Checklist for leaders considering a tiered approach to grantmaking

1. **Identify the problem you are trying to solve:** What is the level of understanding of the particular issue? Is the prompt and question specific enough? What does the prior evidence tell you about the stage of the problem? Would an evaluation of this issue lead to the development of a policy to overcome it? 

2. **Assess internal capacity: **What is the existing evaluation and research capacity of your agency or department? How do you currently evaluate programming, and who is responsible for these activities? What resources would be required to introduce a tiered approach to grantmaking, both at the application assessment point and post-award during implementation? 

3. **Review agency and department standards of evidence: **How have you previously defined success in programming? On the continuum of evidence, where do you place your agency or department as it relates to the problem you would like to evaluate? What would success look like in the particular program you are evaluating?

4. **Aim for strategic alignment: **Is an evidence-based approach to programming embedded into your agency or department’s core strategy? What funding is available to improve internal capacity? What other resources could be sourced to support the growth of institutional evaluation? Does the budget authority support your approach?

## Example Tiered Grant Structure

After establishing where your agency or department sits on the evidence continuum as it relates to the problem you aim to solve, it is important to consider how to structure the phases of advancement. An example of a common framework for tiered deployment for "[Social Spending Innovation Research](http:///h)" follows:

1. **A phase I grant funds the development and feasibility testing of an intervention: **Funded applicants at this level need to show that prior evidence, if any, suggests that the intervention could produce meaningful, positive effects at a reasonable cost, and the project team should demonstrate previous experience implementing an intervention on a similar scale in a community setting. The goal is to get the intervention to be operationally functional: develop  the intervention to the point where it is operating as it was designed to. Test implementation with valid performance metrics or less rigorous methodologies like pre- and post-test studies,   but do not yet employ test more rigorous evaluation designs. 

2. **A phase II grant would fund a rigorous – preferably randomized – evaluation of the intervention, at low cost if possible: **The second phase funding ranges in size, depending on factors like availability of existing, administrative data to measure study outcomes, and the overall cost of the intervention. To keep costs lower, consider measuring key outcomes with existing administrative data, if available, rather than relying on primary data collection. The goal is to measure the intervention’s impact on the primary outcomes of interest, as well as obtain basic, low-cost programmatic data on the effectiveness of implementation. Quasi-experimental design studies can also be used if compare groups or data are not available or overall program sampling is not sufficient for an RCT. 

3. **A phase III grant finds a randomized replication trial of the intervention: **The third phase targets replication of the intervention using a randomized, systematic research design. An RCT could be used to test whether the outcomes from the second phase can be replicated in a different environment, often with a larger population. To enable a longer-term and larger study, the number of grants distributed at this level are usually few, plus relatively few interventions are found to be effective enough in phase II to warrant larger investment. The goals here are to 1) determine whether the positive impacts from prior studies can be reproduced with a new sample of the population in a new setting, and whether they endure long enough to constitute substantive improvements in people’s lives, and 2) identify the reasons that the intervention produced the effects, the conditions and subgroups where it is most effective, and its impacts on a broader set of outcome measures. 

## Introducing new roles

Many agencies and departments introduce a new position to support the expansion of an evidence-driven strategy. Those roles vary across government, but one option is to introduce a Chief Evaluation Officer.

## Introducing a Chief Evaluation Officer: DOL’s Experience [imbed in textbox/sidebar]

Over the last six years, the Department of Labor (DOL) has made significant progress in institutionalizing a culture of evidence and learning. The Chief Evaluation Office (CEO), established in 2010, plays a critical role in developing and maintaining this culture within DOL. As a part of its primary responsibility to manage DOL’s evaluation program, CEO maintains a strong commitment to conducting rigorous, relevant, and independent evaluations. CEO is also committed to identifying and funding research and evaluation priorities established through a collaborative learning agenda process with DOL’s various agencies.

These agencies cover a broad range of topics, from employment and training programs to worker protection and enforcement activities. CEO plays an important role in initiating research that cuts across these agency and program silos. CEO also serves as an "honest broker" on evidence issues within DOL, and its work is not limited to implementing evaluations. CEO actively participates in the performance management and strategic planning processes of the Department, and disseminates the results of their evaluations in formats that enable use by programs and policy makers.

Some of the key capacities that DOL developed to support this important work include:

Hiring staff with sufficient expertise for CEO to manage rigorous evaluations of various methodologies. For example,

* Using behavioral insights, CEO worked with Occupational Safety and Health Administration to implement a large random assignment study that identified an effective way to support establishments that have injury and illness rates above the national average.

* Launching the Clearinghouse for Labor Research and Evaluation (CLEAR), which makes research on labor topics more accessible to practitioners, policymakers, researchers, and the public more broadly, thus increasing the transparency and relevance of the Department’s evaluation efforts.

* Implementing a learning agenda process for the Department, in which CEO collaborates with each agency to identify key evaluation and research priorities.

* Securing the budget authority to set aside a portion of specified program funds (.75% in 2016) to support these evaluations.

* Creating a data analytics unit to support and complement agencies on their analytic needs and work; build data, statistical, and analytical expertise and capacity for the Department; and promote and innovate DOL administrative and public use data.

* Establishing a Department of Labor Evaluation Policy to institutionalize and guide the Department’s evaluation efforts.

Source: OMB, "[Building the Capacity to Produce and Use Evidence](https://www.whitehouse.gov/sites/default/files/omb/budget/fy2017/assets/ap_7_evidence.pdf)"

## Other Considerations for the Use of RCTs or Other Evaluation Designs

RCTs are one rigorous approach to determining whether a policy or intervention is working. The specific intervention should be gauged based on the level of existing evidence and understanding of the problem to determine when an RCT is most appropriate to deploy. Other methods of evaluation may be more appropriate across the stages of tiered grantmaking.

Federal departments and agencies may also consider an exit requirement which allows the grantee to conduct an evaluation that is more rigorous than the evidence cited in the grant application. This offers more credible and extensive evidence for the program. 

## Additional How-To Resources

* For an additional overview of tiered-evidence grant programs, see the GovInnovator’s[ video tutorial](http://govinnovator.com/tiered-evidence-grants) on[ Tiered-Evidence Grantmaking 101](http:///h)** **[6:43].   

* For an example of a framework to assess evidence levels, see the[ regulations](http://www.gpo.gov/fdsys/pkg/FR-2013-03-27/pdf/2013-07016.pdf) for the Investing in Innovation (i3) program at the Department of Education, page 18683, which discuss criteria for development, validation and scale-up grants.  

* For an additional example of evaluating grantees, USAID’s Development Innovation Fund enumerates its approach for evaluating grantees on their[ application site.](https://www.usaid.gov/div/apply/criteria)

# Future States

While there are early standout examples of successfully infusing evidence-based policy and program initiatives -- such as i3, the Social Innovation Fund, and the Development Innovation Ventures – the next steps are to further diffuse the use of this approach into more departments and agencies. What social issues remain misunderstood, and where can a more informed approach be taken to advance our understanding to formulate better policy? These are the frontiers of evidence-based grantmaking. 

Advancing evidence-based policy is also related to ongoing debates in effective program evaluation practices. The most important takeaway as the evidence-based agenda moves forward is the use of the full suite of evaluation tools, understanding that each approach has benefits when deployed in the appropriate context. Just as agencies can look for ways to use existing administrative data to run low-cost RCTs, in some instances they may also consider context-sensitive evaluation mechanisms. Several evaluation scholars (Pawson and Tilley, 1997) have developed alternative evaluation models called CMO (context mechanism=outcome). These approaches focus on "Programs work (have successful ‘outcomes’) only insofar as they introduce the appropriate ideas and opportunities (‘mechanisms’) to groups in the appropriate social and cultural conditions (‘contexts’)."

# Additional Resources

### How-to Resources

In addition, agencies can access a variety of resources to help develop tiered-evidence grant funding mechanisms, including:

* [A Guide to Evidence and Innovation](http://evidence-innovation.findyouthinfo.gov/investingEvidence): Offers useful information about existing tiered-grant funding programs, courtesy of the Interagency Working Group on Youth Programs. 

* [Overview of the Administration’s Evidence-Based Social Policy Initiatives](http://www.brookings.edu/~/media/research/files/articles/2011/4/obama%2520social%2520policy%2520haskins/04_obama_social_policy_haskins.pdf): A report by the Coalition for Evidence-Based Policy.**[from****[ StratInnov 201**5](https://www.whitehouse.gov/sites/default/files/strategy_for_american_innovation_october_2015.pdf)**]**

### Contact

Agencies interested in learning more about tiered-evidence funding approaches can contact David Wilkinson at David_E_Wilkinson@who.eop.gov **[from****[ StratInnov 201**5](https://www.whitehouse.gov/sites/default/files/strategy_for_american_innovation_october_2015.pdf)**]**

### Further Reading and Media:

Websites

* [Results for Americahttp://results4america.org/](http://results4america.org/)

* [Arnold Foundation’s Evidence-Based Policy Innovation Initiative](http://www.arnoldfoundation.org/initiative/evidence-based-policy-innovation/) //[ Coalition for Evidence-Based Policy](http://coalition4evidence.org/) (*Coalition wound down in 2015 but many key documents have not yet migrated to Arnold Fndn site)*

Books

* [Moneyball For Government](http:///h), Jim Nussle & Peter Orszag (2015)

* [Show Me the Evidence](http://www.brookings.edu/research/books/2014/show-me-the-evidence), Ron Haskins (2014)

Podcasts from GovInnovator:

* [Important tools for evidence-based decision making](http://govinnovator.com/margery_turner/): Margery Turner, Urban Institute[ How one Federal agency, the Corporation for National and Community Service (CNCS), strengthened the role of evidence in a key grant program, AmeriCorps](http://govinnovator.com/americorps/): Diana Epstein and Carla Ganiel, CNCS

* [Insights for evidence-based grant making from the Teen Pregnancy Prevention Program](http://govinnovator.com/evelyn_kappeler/): Evelyn Kappeler, U.S. Department of Health and Human Services

* [The federal evidence agenda and lessons for state and local leaders](http://govinnovator.com/ron_haskins/): Ron Haskins, Brookings Institution

* [Strengthening evidence-based grant making at the U.S. Department of Education](http://govinnovator.com/jim_shelton/): Jim Shelton, U.S. Department of Education

* [Becoming an evidence focused grant-making organization](http://govinnovator.com/kelly_fitzsimmons/): Kelly Fitzsimmons, Edna McConnell Clark Foundation

* [Creating outcome focused grant programs known as tiered-evidence grant programs or innovation funds: A video overview](http://govinnovator.com/tiered-evidence-grants/): Andy Feldman

* [Building an evidence base for agency programs](http://govinnovator.com/chris_spera/): Chris Spera, Corporation for National and Community Service

* [Harnessing Silicon Valley funding approaches to drive breakthrough solutions in the public sector](http://govinnovator.com/jeffrey_brown/): Jeffrey Brown, U.S. Agency for International Development (USAID)

* [Strengthening evidence-based grant making at the U.S. Department of Education](http://govinnovator.com/jim_shelton/): Jim Shelton, Deputy Secretary of Education

### Additional specific resources

Related topics, including evidenced-based policy, program evaluation, and research design, may be of interest for those pursuing tiered-evidence grantmaking programs. Selected resources include:

* **[What constitutes strong evidenc**e](http://coalition4evidence.org/wp-content/uploads/2012/12/What-Constitutes-Strong-Evidence-of-Program-Effectiveness-Sputnik-blog.pdf)**? **

* **[U.S. Department of Education: Guide for recognizing and conducting opportunistic experiments**:](http://ies.ed.gov/ncee/pubs/REL2014037/pdf/REL_2014037.pdf)

* **[Key Items to Get Right When Conducting a Randomized Controlled Trials of Social  Program**s](http://www.arnoldfoundation.org/wp-content/uploads/Key-Items-to-Get-Right-When-Conducting-Randomized-Controlled-Trials-of-Social-Programs.pdfhttp:/www.arnoldfoundation.org/wp-content/uploads/Key-Items-to-Get-Right-When-Conducting-Randomized-Controlled-Trials-of-Social-Programs.pdf)

#### Numerous useful checklists and forms from Coalition for Evidence-Based Policy:

* **[Rigorous Program Evaluations on a Budget: How Low-Cost Randomized Controlled Trials Are Possible in Many Areas of Social Polic**y](http://coalition4evidence.org/wp-content/uploads/2012/03/Rigorous-Program-Evaluations-on-a-Budget-March-2012.pdf), 2012 (.pdf, 6 pages)

#### Hierarchy of Study Designs for Evaluating Effectiveness

* **[Which Study Designs Are Capable of Producing Valid Evidence About A Program’s Effectiveness? A Brief Overvie**w](http://coalition4evidence.org/wp-content/uploads/2014/10/Which-Study-Designs-are-Capable-of-Producing-Valid-Evidence-of-Effectiveness.pdf) – October 2014 (.pdf, 4 pages)

* **[Hierarchy of Study Designs for Evaluating the Effectiveness of a STEM Education Project or Practic**e](http://coalition4evidence.org/wp-content/uploads/2009/05/study-design-hierarchy-6-4-09.pdf), 2007 (.pdf, 7 pages + cover)

#### Checklists for Reviewing Evaluations of Program Effectiveness

* **[Which Comparison-Group ("Quasi-Experimental") Study Designs are Most Likely to Produce Valid Estimates of a Program’s Impact?: A Brief Overview and Sample Review For**m](http://coalition4evidence.org/wp-content/uploads/2014/01/Validity-of-comparison-group-designs-updated-January-2014.pdf), 2014 (.pdf, 3 pages + appendix)

* **[Checklist For Reviewing a Randomized Controlled Trial of a Social Program or Project, To Assess Whether It Produced Valid Evidenc**e](http://coalition4evidence.org/wp-content/uploads/2010/02/Checklist-For-Reviewing-a-RCT-Jan10.pdf), 2010 (.pdf, 6 pages + cover)

* **[Two-Page Reviewer Form for Assessing Whether an Intervention is Backed by Strong Evidence of Effectivenes**s](http://coalition4evidence.org/wp-content/uploads/2012/01/Panel-Reviewer-form-Top-Tier-or-Near-Top-Tier-Jan-2012.pdf), 2011 (.pdf, 2 pages)

* **[Evaluation Plan Guidance: a Step by Step Guide to Designing a Rigorous Evaluatio**n](http://www.nationalservice.gov/sites/default/files/documents/SIF%2520Evaluation%2520guidance%25208%25205%25202014.pdf), 2014 (.pdf, 35 pages + appendix) 

#### Guides for *Sponsors* of Research/Evaluation

* **[Practical Evaluation Strategies for Building a Body of Proven-Effective Social Programs: Suggestions for Research and Program Funder**s](http://coalition4evidence.org/wp-content/uploads/2014/05/Practical-Evaluation-Strategies-2013.pdf), 2013 (.pdf, 6 pages)

* **[Increasing the Success of Evaluation Studies in Building a Body of Effective, Evidence-Based Programs: Recommendations of a Peer-Review Pane**l](http://coalition4evidence.org/wp-content/uploads/2013/09/Recommendations-of-peer-review-panel-on-DOL-ETA-evaluation-program-June-2013.pdf), prepared for the Employment and Training Administration, U.S. Department of Labor, 2013 (.pdf, 7 pages)

See additional GovInnovator podcasts on[ advancing evidenced based policy](http://govinnovator.com/topic/#impact) and measuring program impact, including:

* [Performance management and evaluation: A video overview](http://govinnovator.com/perf-mgt-eval-101/): Andy Feldman (video overview) 

* [A provider’s perspective on being part of a rigorous evaluation](http://govinnovator.com/sarah_hurley/): Sarah Hurley, Youth Village

# Related Policies

[Evidence-Based Policymaking Commission Act of 2016](http:///h)

The EBP Commission Act called for a comprehensive study of the data inventory, data infrastructure, database security, and statistical protocols related to federal policymaking and the agencies responsible for maintaining the data. It’s aim was to determine the optimal arrangement for which administrative data on federal programs and tax expenditures, survey data, and related statistical data series may be integrated and made available to facilitate program evaluation, continuous improvement, policy-relevant research, and cost-benefit analyses; make recommendations on how data infrastructure, database security, and statistical protocols should be modified to best fulfill those objectives; and make recommendations on how best to incorporate outcomes measurement, institutionalize randomized controlled trials, and rigorous impact analysis into program design. 

[Economic Report from of President: Evaluation as a Tool for Improving Federal Programs](https://www.whitehouse.gov/sites/default/files/docs/erp_2014_chapter_7.pdf)

Evaluation as a tool for improving federal programs provides an overview of the implementation and use of impact evaluation in Federal programs, with a special focus on the lessons learned so far in this Administration. It begins with a discussion of some challenges inherent in conducting rigorous impact evaluations in government programs. The chapter then focuses on the Administration’s efforts to build and to use evidence, including actions taken on lessons learned from completed evaluations, launching new evaluations in areas where not enough is known, and creating a culture of evidence-building in Federal programs, especially grant programs. 

**OMB Guidance on Evaluation and Data**

[https://www.whitehouse.gov/sites/default/files/docs/erp_2014_chapter_7.pdf](https://www.whitehouse.gov/sites/default/files/docs/erp_2014_chapter_7.pdf)

[https://www.whitehouse.gov/sites/default/files/docs/erp_2014_chapter_7.pdf](https://www.whitehouse.gov/sites/default/files/docs/erp_2014_chapter_7.pdf)[Guidance for Providing and Using Administrative Data for Statistical Purposes – OMB M-14-06 (February 2014)](https://www.whitehouse.gov/sites/default/files/omb/memoranda/2014/m-14-06.pdf)

This Memo from the Director of OMB called for departmental and agency leadership to foster greater collaboration between program and statistical offices; develop strong data stewardship policies and practices around the statistical use of administrative data; require the documentation of quality control measures and key attributes of important administrative datasets; and require the designation of responsibilities and practices through the use of agreements amongst these offices.

**Additional Memoranda and Initiatives from the White House:**

* [Next Steps in the Evidence and Innovation Agenda – OMB M-13-17 (July 2013)](https://www.whitehouse.gov/sites/default/files/omb/memoranda/2013/m-13-17.pdf) 

* [Use of Evidence and Evaluation in the 2014 Budget – OMB M-12-14 (May 2012)](https://www.whitehouse.gov/sites/default/files/omb/memoranda/2012/m-12-14.pdf) 

* [Evaluating Programs for Efficacy and Cost-Efficiency – OMB M-10-32 (July 2010)](https://www.whitehouse.gov/sites/default/files/omb/memoranda/2010/m10-32.pdf) 

* [Increased Emphasis on Program Evaluations-OMB M-10-01 (October 2009)](https://www.whitehouse.gov/sites/default/files/omb/assets/memoranda_2010/m10-01.pdf)

